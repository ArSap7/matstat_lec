%!TEX TS-program = xelatex
\documentclass[12pt, a4paper, oneside]{article}

% Можно вставить разную преамбулу
\input{preamble}

\title{
% \begin{center} 
% \includegraphics[width=0.99\textwidth]{logo.png}
% \end{center}
Метрики качества толоки и метрики качества классификаторов}
%Посиделка 1: схема статистики}
\date{ } %\today}

% Если делаешь конспект, вписывай своё имя прямо сюда!
\author{Ульянкин Ппилиф \\ a.k.a. randomwalk}

\begin{document} % Конец преамбулы, начало файла

\maketitle

% \epigraph{Классный эпиграф}{\textit{автор этого эпиграфа}}

% В этом разделе мы посмотрим на несколько жизненных примеров того, где важно уметь грамотно работать с различными метриками и добиваться от них хороших статистических свойств. 

% \section{Доля плохих показов}

% Сегодня многие поисковые и рекомендательные платформы вроде Яндекса, Google, Youtube и Facebook обеспокоены тем, сколько плохого контента они показывают пользователю. В своих отсчётах\footnote{ \url{https://support.google.com/youtube/answer/2802027?hl=en&ref_topic=9387085} \newline  \url{https://transparency.fb.com/policies/improving/prevalence-metric/}} они пишут о своих оценках \indef{доли плохих показов} и публично обещают эту долю уменьшать с помощью различных методов машинного обучения.

% В этом разделе мы поговорим про то, как можно попытаться оценить долю плохих показов. Мы посмотрим на разные способы и поисследуем статистические свойства этих способов.

% \subsection{Что мы хотим?} 

% Представим себе, что мы youtube. У нас есть правила модерации контента. Например, нельзя показывать порно. Если на платформе кому-то показалось порно --- это плохой показ. Все видео, которые не соответствуют правилам платформы --- плохие. В правилах может быть перечислено довольно много различных нарушений: порно, спам, видео с травмирующим психику, жестоким контентом и т.п.

% Мы не можем модерировать каждый документ. Платформа постоянно растёт, и нам придётся нанимать новых и новых модераторов. Для каждого нарушения мы можем собрать разметку и обучить классификаторы. Часть новых видео классификаторы будут банить без суда и следствия. Часть они будут отправлять на разметку модераторам. Если классификатор хороший, подавляющая часть потока будет оставаться без модерации. Параллельно с классификатором мы можем начать размечать модераторами жалобы и самые вирусные видосы. Это будет какой-то дополнительный сигнал о нарушениях правил платформы.

% Идей, как именно модерировать контент, может быть довольно много. Ни одна из этих идей не будет совершенна, и иногда показы плохих видео всё-равно будут происходить. Хочется построить систему так, чтобы их было как можно меньше. Перед тем, как выстраивать систему, надо построить метрику, которая могла бы долю плохих показов оценить. 

% \begin{center}
% 	\begin{tabular}{c|c|c}
% 		\hline
% 	   видео  & показы & плохое ли видео  \\  \hline 
% 		 $1$  & $show_1$ & $y_1$  \\ 
% 		 $2$  & $show_2$ & $y_2$  \\ 
% 		 $3$  & $show_3$ & $y_3$  \\ 
% 		 $4$  & $show_4$ & $y_4$  \\ 
% 		 \ldots  & \ldots & \ldots  \\ 
% 	\end{tabular}
% \end{center}

% Как выглядит теоретическая величина, которую мы пытаемся оценить?   У нас на платформе есть $N$ видео.  Каждое видео показалось $show_i$ раз. Изначально это видео было либо плохим, $y_i = 1$ либо хорошим, $y_i = 0$. Все показы плохого видео будем считать плохими. При таких предпосылках долей плохих показов будет величина 

% \[ 
% p = \frac{\sum_{i=1}^N y_i \cdot show_i }{\sum_{i=1}^N show_i} = \sum_{i=1}^N y_i \cdot w_i, \quad w_i = \frac{show_i}{\sum_{i=1}^N show_i}. 
% \]

% Величину $y_i$ мы не знаем. Её может выяснить только модератор. Число модераторов ограничено, все видео, которые поступают на платформу, разметить невозможно, так как число видео на платформе, $N$, очень велико. Будем считать, что модератор безошибочно определяет величину $y_i$, если ему показать видео. 

% Нам нужно придумать способ дать модератору для разметки $m$ видео и посчитать оценку доли плохих показов $\hat{p}$ так, чтобы она обладала хорошими статистическими свойствами. А именно, была несмещённой, состоятельной и обладала низкой дисперсией. 

% Зачем нам несмещённость? Потому что мы хотим интерпретировать получающиеся число, как долю показов, где правила платформы нарушены. Зачем нам состоятельность? Мы должны быть уверены, что если мы начнём размечать больше документов, увеличив штат модераторов, мы приблизим нашу оценку к истине. 

% Зачем нам низкая дисперсия? Во-первых, если мы посчитали долю на какой-то день и доверительный интервал для неё накрывает ноль, такая оценка не очень полезна, так как она значимо не отличается от нуля. Во-вторых, если мы улучшили систему модерации, например, обучили новый, более мощный классификатор, доля плохих показов должна упасть. Нам захочется провести АБ-тест и проверить гипотезу об этом.

% Если у процедуры построения оценки высокая дисперсия, модераторам придётся разметить много наблюдений. Если у нас получится уменьшить дисперсию, мы сможем сэкономить мощности модераторов и собрать более маленькую разметку. 

% \subsection{Сэмплирование с повторениями}

% Давайте возьмём много-много бумажек и запишем на них номера наших видео. Первое видео напишем $show_1$ раз, второе видео напишем $show_2$ раз и так далее. После мы свалим получившиеся бумажки в мешок и сделаем выборку из $m$ документов с возвращениями. Вытягиваем карточку, записываем номер видео, возвращаем карточку в мешок. Все выбранные видео отдаём на разметку модераторам. Для каждого видео из выборки модераторы указывают метку $y_i$.  Итоговую оценку доли плохих показов мы можем посчитать как 

% \[
% \hat{p} = \frac{1}{m} \sum_{k = 1}^m y_k. 
% \]

% Такая процедура эквивалента тому, что каждое видео мы сэмплируем с вероятностью $w_i$. Покажем, что оценка будет несмещённой. Пусть $z_i = 1,$ если видео попало в выборку. Такое происходит с вероятностью $w_i$. Тогда 

% \[
% \hat{p} = \frac{1}{m} \sum_{i = 1}^N  y_i \cdot z_i. 
% \]

% Найдём математическое ожидание оценки, будем держать в голове что $y_i$ модераторы определяют безошибочно, и вся случайность идёт из процедуры сэмплирования. Случайная величина $z_i \sim Bin(m, w_i),$ получается 

% \[
% \E(\hat{p}) =  \frac{1}{m} \sum_{i = 1}^N y_i \cdot \E(z_i) =   \frac{1}{m} \sum_{i = 1}^N y_i \cdot m \cdot w_i =  \sum_{i = 1}^N y_i \cdot w_i = p.
% \]

% Оценка оказалась несмещённой. По аналогии найдём дисперсию оценки 

% \[
% \Var(\hat{p}) =  \frac{1}{m^2} \sum_{i = 1}^N y_i \cdot \Var(z_i) =   \frac{1}{m^2} \sum_{i = 1}^N y_i \cdot m \cdot w_i \cdot (1 - w_i) =  \frac{\sum_{i = 1}^N y_i \cdot w_i \cdot (1 - w_i)}{m}.
% \]


% \subsection{Сэмплирование без повторений и смещение}

% В чём минус подхода выше? У нас есть квота, которая заключается в том, что модераторы в сутки могут разметить $m$ видео. Если мы сэмплируем $m$ видео с повторениями, реальное количество видео, попавшее на разметку, отличается от $m$. Оно тоже представляет из себя случайную величину. 

% Если мы хотим занять всех модераторов делом, нам придётся показывать им какие-то из видео по несколько раз, в чём нет никакого смысла. Мы можем попробовать настроить процедуру сэмплирования таким образом, чтобы каждый день получать в среднем $m$ видео на разметку, но тогда этот поток будет непредсказуемым. Потенциально видео на разметку может придти больше нормы, и модераторы не справятся в срок. Хочется, чтобы поток был детерминирован, а ресурсы чётко контролировались. \indef{Выход, который позволяет этого достичь --- сэмплирование без повторений.} 

% Представим себе, что у нас есть случайная величина $X$, которая принмает пять значений с вероятностями 

% \begin{center}
% 	\begin{tabular}{|c|c|c|}
% 		& $X$  &  $x_1$ &  $x_2$ &  $x_3$ &  $x_4$ &  $x_5$  \\  \hline 
% 		& $\PP(X = x)$  &  $\frac{1}{2}$ &  $\frac{1}{4}$ &  $\frac{1}{8}$ &  $\frac{1}{16}$ &  $\frac{1}{16}$
% 	\end{tabular}
% \end{center}

% Если мы делаем выборку с повторениями, как часто туда будет попадать элемент $x_1$? Элемент попадает в выборку с вероятностью $\frac{1}{2}.$ Будем вытаскивать из выборки элементы до тех пор, пока $x_1$ не окажется в наших руках. Номер попытки, начиная с которой $x_1$ окажется у нас, имеет геометрическое распределение. 

% Если $Y \sim Geom(p),$ тогда $\E(Y) = \frac{1}{p}.$ Получается, элемент $x_1$ окажется в нашей выборке в среднем на второй попытке. Учитывая, что мы делаем выборку с повторениями, тогда каждый второй элемент в ней будет принимать значение $x_1,$  каждый четвёртый будет принимать значение $x_2$, каждый восьмой будет принимать значение $x_3$ и так далее. Обратим внимание, что если мы делаем выборку из трёх элементов без повторений, то чаще всего мы будем работать с выборкой $x_1, x_2, x_3$. 

% В ситуации с видео, мы сэмплируем их пропорционально числу показов. В числе показов может быть очень сильный перекос. Какие-то видео показываются в рекомендательной системе десятки раз, а какие-то вирусятся и прорываются в тренды. 

% Представим себе, что у нас есть $11$ видео. В одном из них есть порно. Это видео показалось один раз. Девять хороших видео показались по одному раз. Одно из видео было с котиком. Оно завирусилось и показалось $100$ раз. Теоретическая доля плохих показов составляет $\frac{1}{110}.$

% Пусть мы делаем выборку из двух элементов с повторениями и по ней оцениваемм долю плохих показов. Наблюдения мы выбираем с вероятностью пропорциональной числу показов. Давайте найдём распределение такой оценки. Мы можем оказаться в трёх ситуациях  

% \begin{center}
% 	\begin{tabular}{|c|c|c|}
% 		\hline
% 	хор. хор.& хор. порно  & порно порно  \\  \hline 
% 	$\left(1 - \frac{1}{110} \right) \cdot \left(1 - \frac{1}{110} \right)$  &  $\left(1 - \frac{1}{110} \right) \cdot \frac{1}{110}$ & $\frac{1}{110} \cdot \frac{1}{110}$
% 	\end{tabular}
% \end{center}





% Два видео с порно показались по 1 разу, ещё 7 хороших видео тоже показались по 1 разу. На одном видео был котик. Оно завирусилось и показалось сто раз. Если мы будем делать сэмпл с повторениями, мы постоянно будем нарываться на видео с котиком. Порно к нам в выборку будет попадаться довольно редко. Если мы будем делать сэмпл без повторения, тогда видео с котиками мы возьмём только один раз. Вероятность попадания в выборку порно возрастает. Тем не менее, у него было довольно мало показов.  Получается, что мы искуственно завышаем долю показов порно.  




% \subsection{Как исправить смещение: веса}

% На выборку без повторений размера $m$ можно смотреть следующим образом: мы генерируем выборку с возвращением до тех пор, пока количество уникальных элементов не достигнет числа $m$. При этом, если мы берём на каком-то шаге элемент уже выбранный ранее, мы не включаем его в выборку, а только отдельно запоминаем где-нибудь счётчик числа вхождений этого элемента $c_i$.

% При таком подходе несмещённую оценку доли плохого можно записать как 

% \[
% \hat{p} = \frac{\sum_{a_i \in Sample} c_i \cdot y_i}{\sum_{a_i \in Sample} c_i.}
% \]

% Если выборка имеет сильно неравномерные веса, то нам придётся довольно долго генерировать элементы с возвращением, пока мы наберём необходимое количество уникальных видео. Поэтому вместо того, чтобы накапливать счётчики вхождений, мы рассчитаем их математическое ожиданий по всем сгенерированным выборкам без повторений, имеющим такое же упорядоченное множество элементов $w_i = \E(c_i).$






% \subsection{Как исправить смещение: сэмплирование с остановкой}




% \subsection{Понижение дисперсии с помощью классификатора}


% \section{Метрики качества классификатора и толоки}

Описание метрик качества делается на примере классификатора спама. Качество всех толокерских проектов оценивается аналогичным способом. Модераторам даётся сбалансированная выборка из статей, которые получили из толоки ок и не ок.

% Рассмотрим ещё один интересный сюжет. 
Пусть у нас есть классификатор спама. Он банит статьи, которые люди пишут в рекомендательный фид. Спамеры постоянно эволюционируют. Они перебирают разные варианты написания текстов, чтобы обойти классификатор. Из-за этого нам надо регулярно собирать разметку и регулярно переобучать классификатор спама. Иначе его качество деградирует и спамеры победят. Будем считать, что спам возникает в потоке довольно редко, в $1\%$ случаев.

Чтобы понимать, когда классификатор начал деградировать, модератор Настя каждый день размечает $100$ забаненных наблюдений и $100$ не забаненных. На основе этих двухсот наблюдений рисуется матрица ошибок, а затем вычисляются точность, полнота, доля несправедливо забаненных хороших статей и доля верно данных ответов. 

\indef{Правда ли, что все эти метрики будут отражать реальность? Правда ли, что все они несмещённые? Если метрики смещены, то завышены они или занижены?} Вспомним как выглядит матрица ошибок и основные метрики бинарной классификации

\begin{center}
	\begin{tabular}{|c|c|c|}
		\hline
		& $y=0$  &  $ y = 1$ \\  \hline 
		$\hat y = 0$ & $TN$ & $FN$ \\ \hline 
		$\hat y = 1$ & $FP$ & $TP$ \\ \hline
	\end{tabular}
\end{center}

Буквой $y$ обозначается реальная метка класса, буквой $\hat y$ наш прогноз. То есть, по строкам отложены прогнозы, по столбцам реальные значения. 

Доля правильных ответов, $Accuracy$ показывает нам, сколько документов мы классифицировали верно. Точность, $Precision$ говорит нам о том, какую долю мы забанили справедливо. 

\begin{equation} 
\quad Precision = \frac{TP}{TP + FP} \qquad Accuracy = \frac{TP + TN}{TP + TN + FP + FN}.
\end{equation}

Ошибка первого рода, $FPR$ говорит нам о том какую долю хороших документов мы ошибочно забанили. Ошибкой второго рода будет доля плохих документов, которую мы не смогли найти. Полнота, $Recall$ --- это величина обратная ошибке второго рода. Она описывает мощность нашего классификатора и говорит о том, какую долю плохого мы нашли

\begin{equation} 
Recall = \frac{TP}{TP + FN} \qquad  FPR = \frac{FP}{FP + TN}. 
\end{equation}

Все эти метрики считаются в разных разрезах. Точность считается по строкам, в её контексте нас волнуют прогнозы классификатора. Полнота и ошибка первого рода считаются по столбцам, нас волнуют реальные метки. Порассуждаем про несмещённость оценок с точки зрения интуиции, затем получим формулы. 

Когда мы отдаём Насте $100$ забаненных и $100$ хороших статей, мы делаем срезы по строчкам. Мы знаем прогнозы классификаторов. При расчёте $Precision$ мы используем строчку $\hat y = 1$. Из-за этого искажений не возникает и оценка получается корректной. 

Когда мы хотим оценить $FPR$ и $Recall$, нам нужно смотреть на столбцы. Выборка формировалась по строкам. Баланс в рамках столбцов при формировании выборки не был учтён. На потоке спам встречается редко, в $1\%$ случаев. У нас, если классификатор работает хорошо, его будет около $50\%$. Отсюда возникнут искажения. Доля правильных ответов тоже окажется искажена, так как она ориентируется на всю таблицу целиком. 

Как можно было бы исправить проблему? Логично сделать это на уровне сэмплирования. Давайте будем давать Насте не $100$ плохого и $100$ хорошего, а просто $1000$ случайных статей. Тогда все метрики будут соответствовать потоку. Проблема такого подхода в том, что спам встречается только в $1\%$ случаев, и в этой большой выборке, его может просто-напросто не оказаться. Неужели Насте придётся размечать многие тысячи наблюдений, чтобы корректно оценить полноту классификатора?  

К счастью, нет. Метрики, полученные первым способом, можно скорректировать так, чтобы они отражали свойства потока статей в рекомендательную систему. Мы всегда можем корректно оценить доли, расположенные по строкам. Отсюда можно получить формулы пересчёта. 

Пусть в природе существует $N_{bad}$ плохих документов и $N_{ok}$ хороших. Обозначим как $TN, FP, FN, TP$ элементы матрицы ошибок, которые соответствуют потоку.  Пусть $s_1$ --- число наблюдений, которое забанил классификатор, а $s_0$ --- число наблюдений, которые он не забанил. Тогда матрица ошибок будет выглядеть как 

\begin{center}
	\begin{tabular}{|c|c|c|c|}
		\hline
		& $y=0$  &  $ y = 1$ & \\  \hline 
		$\hat y = 0$ & $TN$ & $FN$ & $s_0 = TN + FN$\\ \hline 
		$\hat y = 1$ & $FP$ & $TP$ & $s_1 = FP + TP$\\ \hline
	\end{tabular} 
\end{center}

Классификатор забанил $s_1$ статей. Среди них мы отобрали $n_1$ статей. Классификатор не стал банить $s_0$ статей. Среди них мы отобрали $n_0$ статей. Модератор Настя разметила $n_0 + n_1$ статей и получила ещё одну матрицу ошибок 

\begin{center}
    \begin{tabular}{|c|c|c|c|}
		\hline
		& $y=0$  &  $ y = 1$ & \\  \hline 
		$\hat y = 0$ & $TN'$ & $FN'$ & $n_0 = 100$ \\ \hline 
		$\hat y = 1$ & $FP'$ & $TP'$ & $n_1 = 100$ \\ \hline
	\end{tabular}
\end{center}

Попробуем связать наши матрицы ошибок между собой. Мы можем сделать это в разрезе строк, так как по ним мы контролируем все количества. Мы знаем $s_0, s_1$, $n_0, n_1$, $TP', FN', TN', FP',$ но не знаем $TP, FP, FN, TN.$ 

Какую долю потока статей классификатор забанил правильно? Конечно же $\frac{TP}{s_1}.$ При этом среди отобранных нами $n_1$ наблюдений он корректно забанил $\frac{TP'}{n_1}$ наблюдений. Выходит, что 

\[
\frac{TP}{s_1} = \frac{TP'}{n_1}  \Rightarrow TP =  \frac{s_1}{n_1} TP'.
\]

По аналогии по строкам между собой должны соотноситься и все остальные доли. Отсюда мы получаем оставшиеся три формулы пересчёта 

\begin{equation*} 
\begin{aligned} 
&\frac{FP}{s_1} = \frac{FP'}{n_1} & \Rightarrow &  FP =  \frac{s_1}{n_1} FP' \\
&\frac{TN}{s_0} = \frac{TN'}{n_0} &  \Rightarrow &  TN =  \frac{s_0}{n_0} TN' \\ 
&\frac{FN}{s_0} = \frac{FN'}{n_1} & \Rightarrow &  FN =  \frac{s_0}{n_0} FN'.
\end{aligned} 
\end{equation*} 

Давайте теперь посмотрим в какую сторону будут искажены метрики классификации, если мы будем рассчитывать их без коррекции на процедуру сэмплирования. Убедимся, что точность не изменяется

\[
Precision = \frac{TP}{TP + FP} = \frac{\dfrac{s_1}{n_1} \cdot TP'}{\dfrac{s_1}{n_1} \cdot TP' + \dfrac{s_1}{n_1} \cdot  FP'} = \frac{TP'}{TP' + FP'} = Precision'.
\]

Из-за того, что по строкам коррекция одинаковая, метрика неискажена. Посмотрим на полноту 

\[
Recall = \frac{TP}{TP + FN} = \frac{\dfrac{s_1}{n_1} \cdot TP'}{\dfrac{s_1}{n_1} \cdot TP' + \dfrac{s_0}{n_0} \cdot  FN'} = \frac{TP'}{TP' + \dfrac{s_0 n_1}{n_0 s_1} FN'} < Recall'.
\]

В нашем случае $n_0 = n_1$. Будем считать, что пороги подобраны так, что $s_0$ заметно больше $s_1$. Классификатор не перебанивает. Тогда величина $\frac{s_0 n_1}{n_0 s_1} = \frac{s_0}{s_1}$ будет принимать значение больше единицы. Выходит, что знаменатель из-за коррекции сильно возрастёт, каждая ошибка $FN$ будет нам стоить дороже и полнота уменьшится. До коррекции полнота классификатора была сильно завышена. Ошибка первого рода тоже оказывается завышена

\[
FPR = \frac{FP}{FP + TN} = \frac{FP'}{FP' + \dfrac{s_0 n_1}{n_0 s_1} TN'} < FPR'.
\]

Доля правильных ответов также оказывается завышена

\[
Accuracy = \frac{TP + TN}{TP + FP + TN +FN} = \frac{TP' + \dfrac{s_0}{s_1} TN'}{TP' + FP' + \dfrac{s_0}{s_1}\left( TN' + FN' \right)} < Accuracy'.
\]

Стоит обратить отдельное внимание на то, что $FN$ ошибки из-за редкости спама в потоке в принципе встречаются довольно редко. Возможно, что для их поиска нужно выстроить какую-то дополнительную процедуру, которая поможет намайнить таких ошибок. А дальше метрики снова нужно будет перевзвесить на поток документов. 


% \section*{Почиташки} 

% \todo[inline]{Сюда список литературы к лекции}


\end{document}

Винни-Пух решает задачу бинарной классификации. Когда он строил матрицу ошибок, он рещил что будет использовать в качестве 1 первый класс. 

Как будут связаны между собой $Recall_1, Recall_2, FPR_1, FPR_2$? С какими величинами по аналогии будут связаны $Precision_1$ и $Precision_2$?

ans: Recall_1 + FPR_2 = 1, Recall_2 + FPR_1 = 1. FPR_2 - сколько 1 мы ошибочнообъявили как 2. Recall_1 - сколько из класса 1 ммы в принципе нашли



