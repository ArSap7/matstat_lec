%!TEX TS-program = xelatex
\documentclass[12pt, a4paper, oneside]{article}

% Можно вставить разную преамбулу
\input{preamble}

\title{
\begin{center} 
\includegraphics[width=0.99\textwidth]{logo.png}
\end{center}

Посиделка 1: схема статистики}
\date{ } %\today}

% Если делаешь конспект, вписывай своё имя прямо сюда!
\author{Ульянкин Ппилиф}

\begin{document} % Конец преамбулы, начало файла

\maketitle

\epigraph{Классный эпиграф}{\textit{автор этого эпиграфа}}

В этом разделе мы посмотрим на несколько жизненных примеров того, где важно уметь грамотно работать с различными метриками и добиваться от них хороших статистических свойств. 

\section{Доля плохих показов}

Сегодня многие поисковые и рекомендательные платформы вроде yanfex, google, youtube и facebook обеспокоены тем, сколько плохого контента они показывают пользователю. В своих отсчётах\footnote{Пример отчёта youtube: \url{https://support.google.com/youtube/answer/2802027?hl=en&ref_topic=9387085} \newline  Пример отчёта facebook: \url{https://transparency.fb.com/policies/improving/prevalence-metric/}} они пишут о своих оценках \indef{доли плохих показов} и публично обещают эту долю уменьшать с помощью различных методов машинного обучения.

В этом разделе мы поговорим про то, как можно попытаться оценить долю плохих показов. Мы посмотрим на разные способы и поисследуем статистические свойства этих способов. Обычно хочется, чтобы оценка была несмещённой, состоятельной и  чтобы дисперсия у неё была как можно меньше. 

\subsection{Что мы хотим?} 

Представим себе, что мы youtube. У нас есть правила модерации контента. Например, нельзя показывать порно. Если на платформе кому-то показалось порно --- это плохой показ. Все видео, которые не соответствуют правилам платформы --- плохие. В правилах может быть перечислено довольно много различных нарушений: спам, детский контент, видео с травмирующим психику, жестоким контентом и т.п.

Мы не можем модерировать каждый документ. Платформа постоянно растёт, и нам придётся нанимать новых и новых модераторов. Для каждого нарушения мы можем собрать разметку и обучить классификаторы. Часть потока новых видео классификаторы будут банить без суда и следствия. Часть они будут отправлять на разметку модераторам. Если классификатор хороший, подавляющая часть потока будет оставаться без модерации. Параллельно с классификатором мы можем начать размечать модераторами жалобы и самые вирусные видосы. Это будет какой-то дополнительный сигнал о нарушениях правил платформы. 

Идей, как именно модерировать контент, может быть довольно много. Ни одна из этих идей не будет совершенна, и иногда показы плохих видео всё-равно будут происходить. Хочется построить систему так, чтобы их было как можно меньше. Перед тем, как выстраивать систему, надо построить метрику, которая могла бы долю плохих показов оценить. 

\begin{center}
	\begin{tabular}{l|c|c}
		\hline
	   видео  & показы & плохое ли видео  \\  \hline 
		 $1$  & $show_1$ & $y_1$  \\ 
		 $2$  & $show_2$ & $y_2$  \\ 
		 $3$  & $show_3$ & $y_3$  \\ 
		 $4$  & $show_4$ & $y_4$  \\ 
		 \ldots  & \ldots & \ldots  \\ 
	\end{tabular}
\end{center}

Как выглядит теоретическая величина, которую мы пытаемся оценить?   У нас на платформе есть $N$ видео.  Каждое видео показалось $show_i$ раз. Изначально это видео было либо плохим, $y_i = 1$ либо хорошим, $y_i = 0$. Все показы плохого видео будем считать плохим.  При таких предпосылках долей плохих показов будет величина 

\[ 
p = \frac{\sum_{i=1}^N y_i \cdot show_i }{\sum_{i=1}^N show_i} = \sum_{i=1}^N y_i \cdot w_i, \qquad w_i = \frac{show_i}{\sum_{i=1}^N show_i}. 
\]

Величину $y_i$ мы не знаем. Её может выяснить только модератор. Число модераторов ограничено, все видео, которые поступают на платформу, разметить невозможно, так как число видео на платформе, $N$ очень велико. Будем считать, что модератор безошибочно определяет величину $y_i$, если ему показать видео. 

Нам нужно придумать способ дать модератору для разметки $m$ видео и посчитать оценку доли плохих показов $\hat{p}$ так, чтобы она обладала хорошими статистическими свойствами. А именно, была несмещённой, состоятельной и обладала низкой дисперсией. 

Зачем нам несмещённость? Потому что мы хотим интерпретировать получающиеся число, как долю показов, где правила платформы нарушены. Зачем нам состоятельность? Мы должны быть уверены, что если мы начнём размечать больше документов, увеличив штат модераторов, мы приблизим нашу оценку к истине. 

Зачем нам низкая дисперсия? Во-первых, если мы посчитали долю на какой-то день и доверительный интервал для неё накрывает ноль, такая оценка не очень полезна, так как она значимо не отличается от нуля. Во-вторых, если мы улучшили систему модерации, например, обучили новый, более мощный классификатор, доля плохих показов должна упасть. Нам захочется провести АБ-тест и проверить гипотезу об этом.

Если у процедуры построения оценки высокая дисперсия, модераторам придётся разметить много наблюдений. Если у нас получится уменьшить дисперсию, мы сможем сэкономить мощности модераторов и собрать более маленькую разметку. 

\subsection{Сэмплирование с повторениями}

Давайте возьмём много-много бумажек и запишем на них номера наших видео. Первое видео напишем $show_1$ раз, второе видео напишем $show_2$ раз и так далее. После мы свалим получившиеся бумажки в мешок и сделаем выборку из $m$ документов с возвращениями. Вытягиваем карточку, записываем номер видео, возвращаем карточку в мешок. Все выбранные видео отдаём на разметку модераторам. Для каждого видео из выборки модераторы указывают метку $y_i$.  Итоговую оценку доли плохих показов мы можем посчитать как 

\[
\hat{p} = \frac{1}{m} \sum_{v \in sample} y_v. 
\]

\subsection{Сэмплирование без повторений}

\subsection{Сэмплирование без повторений с остановкой}

\subsection{Понижение дисперсии с помощью классификатора}


\section{Метрики качества классификатора}

Рассмотрим ещё один интересный сюжет. Пусть у нас есть классификатор спама. Он банит документы. Спамеры постоянно эволюционирую. Они перебирают разные варианты написания текстов, чтобы обойти классификатор. Из-за этого нам надо регулярно собирать разметку и регулярно переобучать классификатор спама. Иначе его качество деградирует и спамеры победят. Будем считать, что спам возникает в потоке довольно редко, в $1\%$ потока.

Чтобы понимать, когда классификатор начал деградировать, модератор Настя каждый день размечает $100$ забаненных наблюдений и $100$ не забаненных. На основе этих двухсот наблюдений рисуется матрица ошибок, а затем вычисляются точность, полнота, FPR и доля верно данных ответов. 

\indef{Правда ли, что все эти метрики будут отражать реальность? Правда ли, что все они несмещённые? Если метрики смещены, то завышены они или занижены?} Вспомним как выглядит матрица ошибок и основные метрики бинарной классификации

\begin{center}
	\begin{tabular}{|c|c|c|}
		\hline
		& $y=0$  &  $ y = 1$ \\  \hline 
		$\hat y = 0$ & $TN$ & $FN$ \\ \hline 
		$\hat y = 1$ & $FP$ & $TP$ \\ \hline
	\end{tabular}
\end{center}

Доля правильных ответов, $Accuracy$ показывает нам сколько документов мы классифицировали верно. Точность, $Precision$ говорит нам о том, какую долю мы забанили справедливо. 

\begin{equation} 
\quad Precision = \frac{TP}{TP + FP} \qquad Accuracy = \frac{TP + TN}{TP + TN + FP + FN}
\end{equation}

Ошибка первого рода, $FPR$ говорит нам о том какую долю хороших документов мы ошибочно забанили. Полнота, $Recall$ --- это величина обратная ошибке второго рода. Она описывает мощность нашего классификатора и говорит о том какую долю плохого мы нашли

\begin{equation} 
Recall = \frac{TP}{TP + FN} \qquad  FPR = \frac{FP}{FP + TN} 
\end{equation}

Все эти метрики считаются в разных разрезах. Точность считается по строкам, нас волнуют прогнозы классификатора. Полнота и ошибка первого рода считаются по столбцам, нас волнуют реальные метки. Порассуждаем про несмещённость оценок с точки зрения интуиции, затем получим формулы. 

Когда мы отдаём Насте $100$ забаненных и $100$ хороших статей, мы делаем срезы по строчкам. Мы знаем прогнозы классификаторов. При расчёте $Precision$ мы используем строчку $\hat y = 1$. Из-за этого искажений не возникает и оценка получается корректной. 

Когда мы хотим оценить $FPR$ и $Recall$, нам нужно смотреть на столбцы. Выборка формировалась по строкам. Баланс в рамках столбцов при формировании выборки не был учтён. На потоке спам встречается редко, в $1\%$ случаев. У нас, если классификатор работает хорошо, его будет около $50\%$. Отсюда возникнут искажения в рассматриваемых двух метриках. Доля правильных ответов тоже окажется искажена, так как она ориентируется на всю таблицу целиком. 

Как можно было бы исправить проблему? Логично сделать это на уровне сэмплирования. Давайте будем давать Насте не $100$ плохого и $100$ хорошего, а просто $1000$ случайных статей. Тогда все метрики будут соответствовать потоку. Проблема такого подхода в том, что спам встречается только в $1\%$ случаев, и в этой большой выборке, его может просто-напросто не оказаться. Неужели Насте придётся размечать многие тысячи наблюдений, чтобы корректно оценить полноту классификатора?  

К счастью, нет. Метрики, полученные первым способом, можно скорректировать так, чтобы они отражали свойства потока документов. Давайте выведем эту корректировку. 

Пусть $s_1$ --- число наблюдений, которое забанил классификатор, а $s_0$ --- число наблюдений, которые он не забанил. Пусть $\alpha = $

\begin{center}
	\begin{tabular}{|c|c|c|c}
		\hline
		& $y=0$  &  $ y = 1$ & \\  \hline 
		$\hat y = 0$ & $TN$ & $FN$ & $s_0$\\ \hline 
		$\hat y = 1$ & $FP$ & $TP$ & $s_1$\\ \hline
	\end{tabular} \hspace{2cm}	\begin{tabular}{|c|c|c|}
		\hline
		& $y=0$  &  $ y = 1$ \\  \hline 
		$\hat y = 0$ & $TN$ & $FN$ \\ \hline 
		$\hat y = 1$ & $FP$ & $TP$ \\ \hline
	\end{tabular}
\end{center}

\todo[inline]{wow}


Поправки готовы, чтобы сделать модераторскую матрицу ошибок релевантной потоку,


Давайте теперь посмотрим в какую сторону будут искажены метрики классификации, если мы будем рассчитывать их без коррекции на процедуру сэмплирования. Убедимся, что точность не изменяется

\[
Precision' = \frac{TP'}{TP' + FP'} = \frac{\dfrac{s_1}{n_1} \cdot TP}{\dfrac{s_1}{n_1} \cdot TP + \dfrac{s_1}{n_1} \cdot  FP} = \frac{TP}{TP + FP} = Precision.
\]

Из-за того, что по строка коррекция одинаковая, метрика неискажена. Посмотрим на полноту 

\[
Recall' = \frac{TP'}{TP' + FN'} = \frac{\dfrac{s_1}{n_1} \cdot TP}{\dfrac{s_1}{n_1} \cdot TP + \dfrac{s_0}{n_0} \cdot  FN} = \frac{TP}{TP + \dfrac{s_0 n_1}{n_0 s_1} FN} < Recall.
\]

Будем считать, что $n_0 = n_1$ и $s_0$ заметно больше $s_1$. Классификатор не перебанивает. Тогда величина $\dfrac{s_0 n_1}{n_0 s_1} = \frac{s_0}{s_1} >> 1$. Выходит, что знаменатель из-за коррекции сильно возрастёт, каждая ошибка $FN$ будет нам стоить дороже и полнота уменьшится. До коррекции полнота классификатора была сильно завышена. Ошибка первого рода тоже оказывается завышена

\[
FPR' = \frac{FP'}{FP' + TN'} = \frac{FP}{FP + \dfrac{s_0 n_1}{n_0 s_1} TN} < FPR.
\]

Доля правильных ответов также оказывается завышена

\[
Accuracy' = \frac{TP' + TN'}{TP' + FP' + TN' +FN'} = \frac{TP + \dfrac{s_0}{s_1} TN}{TP + FP + \dfrac{s_0}{s_1}\left( TN + FN \right)} < Accuracy.
\]

Стоит обратить отдельное внимание на то, что $FN$ ошибки из-за редкости спама в потоке в принципе встречаются довольно редко. Возможно, что для их поиска нужно выстроить какую-то дополнительную процедуру.



\section*{Почиташки} 

\todo[inline]{Сюда список литературы к лекции}


\end{document}

Винни-Пух решает задачу бинарной классификации. Когда он строил матрицу ошибок, он рещил что будет использовать в качестве 1 первый класс. 

Как будут связаны между собой $Recall_1, Recall_2, FPR_1, FPR_2$? С какими величинами по аналогии будут связаны $Precision_1$ и $Precision_2$?

ans: Recall_1 + FPR_2 = 1, Recall_2 + FPR_1 = 1. FPR_2 - сколько 1 мы ошибочнообъявили как 2. Recall_1 - сколько из класса 1 ммы в принципе нашли



