%!TEX TS-program = xelatex
\documentclass[12pt, a4paper, oneside]{article}

% Можно вставить разную преамбулу
\input{preamble}

\title{
\begin{center} 
\includegraphics[width=0.99\textwidth]{logo.png}
\end{center}

Посиделка 7: энтропия и её друзья}
\date{ } %\today}

% Если делаешь конспект, вписывай своё имя прямо сюда!
\author{Ульянкин Ппилиф}

\begin{document} % Конец преамбулы, начало файла

\maketitle

\epigraph{Цитата про средние и асимптотику}{\textit{Её автор}}

В этой лекции мы поговорим про энтропию, дивергенцию Кульбака-Лейблера и информационные критерии. В конце мы обсудим как это всё помогает нам при работе TSNE. 

\section{Энтропия} 

Приятно было бы начать этот раздел с картинки. Давайте посмотрим на распределение двух случайных величин. Какая из них предсказуемее: левая или правая? 

\begin{figure}[H]
\begin{minipage}[H]{0.49\linewidth}
	\begin{tikzpicture}
	% оси
	\draw [->] (-1.8,0) -- (4,0);
	\draw [->] (0,-0.2) -- (0,3.5);
	% график
	\draw [blue, thick, domain=0:3] plot (\x, 2);
	\draw [->, blue, thick] (-1.8,0)--(-0.05,0);
	\draw [<-, blue, thick] (3.05,0)--(4,0);
	\draw [blue, thick,dashed] (3,0)--(3,2);
	% точки
	\draw[fill,blue] (3,2) circle [radius=0.03];f
	\draw[fill,blue] (0,2) circle [radius=0.03];
	% подписи
	\node [below] at (0.2,0) {0};
	\node [below] at (3,0) {3};
	\node [left] at (0,2) {$\frac{1}{3}$};
	\node [below right] at (4,0) {$y$};
	\node [left] at (0,3.3) {$f_Y(y)$};
	\end{tikzpicture}
\end{minipage}
\hfill
\begin{minipage}[H]{0.49\linewidth}
	\begin{tikzpicture}
	% оси
	\draw [->] (-2.8,0) -- (2,0);
	\draw [->] (-1,-0.2) -- (-1,3.5);
	% график12
	\draw [blue, thick, domain=0:1] plot (\x, {12*\x*\x*(1-\x)});
	\draw [blue, thick] (-2.8,0)--(0,0);
	\draw [blue, thick] (1,0)--(2,0);
	%\draw [blue, thick,dashed] (2,0)--(2,2);
	% точки
	%\draw[fill,blue] (2,2) circle [radius=0.03];
	%\draw[fill,blue] (0,2) circle [radius=0.03];
	% подписи
	\node [below] at (0,0) {1};
	\node [below] at (1,0) {2};
	% \node [left] at (0,1) {1};
	\node [below right] at (2,0) {$z$};
	\node [left] at (-1,3.3) {$f_Z(z)$};
	\end{tikzpicture}
\end{minipage}
\end{figure} 

Случайная величина $Z$ (правая) сконцентрирована на довольно узком отрезке. Вероятность того, что она выпадет за его пределами не очень большая. Случайная величина $Y$ (левая) сконцентрирована на широком отрезке. Она равновероятно может выскочить из любой его части. Логика подсказывает, что она непредсказуемее. Если мы решим спрогнозировать эти две случайные величины, правый случай нам будет обуздать легче. Ошибка, которую мы будем допускать, окажется меньше чисто из-за пикообразной природы этой случайной величины. 

Давайте теперь посмотрим на ещё две картинки. Какая непредсказуемее: левая или правая? 

\begin{figure}[H]
	\begin{minipage}[H]{0.49\linewidth}
		\begin{tikzpicture}
		% оси
		\draw [->] (-1.8,0) -- (4,0);
		\draw [->] (0,-0.2) -- (0,3.5);
		% график
		\draw [blue, thick, domain=0:2] plot (\x, 2);
		\draw [->, blue, thick] (-1.8,0)--(-0.05,0);
		\draw [<-, blue, thick] (2.05,0)--(4,0);
		\draw [blue, thick,dashed] (2,0)--(2,2);
		% точки
		\draw[fill,blue] (2,2) circle [radius=0.03];
		\draw[fill,blue] (0,2) circle [radius=0.03];
		% подписи
		\node [below] at (0.2,0) {0};
		\node [below] at (2,0) {1};
		\node [left] at (0,2) {1};
		\node [below right] at (4,0) {$y$};
		\node [left] at (0,3.3) {$f_Y(y)$};
		\end{tikzpicture}
	\end{minipage}
	\hfill
	\begin{minipage}[H]{0.49\linewidth}
		\begin{tikzpicture}
% оси
\draw [->] (-1.8,0) -- (4,0);
\draw [->] (0,-0.2) -- (0,3.5);
% график
\draw [blue, thick, domain=1:3] plot (\x, 2);
\draw [->, blue, thick] (-1.8,0)--(0.96,0);
\draw [<-, blue, thick] (3.05,0)--(4,0);
\draw [blue, thick,dashed] (3,0)--(3,2);
\draw [blue, thick,dashed] (1,0)--(1,2);
% точки
\draw[fill,blue] (3,2) circle [radius=0.03];f
\draw[fill,blue] (1,2) circle [radius=0.03];
% подписи
\node [below] at (1.2,0) {1};
\node [below] at (3,0) {2};
\node [left] at (0,2) {1};
\node [below right] at (4,0) {$z$};
\node [left] at (0,3.3) {$f_Z(z)$};
\end{tikzpicture}
	\end{minipage}
\end{figure} 

Случайные величины $Y$ и $Z$ отличаются друг от друга только отрезком. Одна распределена от $0$ до $1$, вторая от $1$ до $2$. Их форма одинакова. Они принимают разные значения, но одинаково непредсказуемо. Их одинаково сложно спрогнозировать.

Для того, чтобы улавливать такие вещи придумали специальную меру. Она называется энтропией.  \indef{Энтропия} --- это мера непредсказуемости случайной величины $Y$, это то количество информации, которое я получаю, наблюдая случайную величину $Y$.  Она никак не опирается на те значения, которые принимает случайная величина и для дискретного случая определяется  как 

\[ H(Y) = \E(- \ln \PP(Y)). \]

Для непрерывного случая  энтропия определяется как 

\[ H(Y) = \E(- \ln f_Y(y)). \]

Попробуем немного пожить с энтропией. После того, как мы освоимся, можно будет поглубже обсудить её смысловую составляющую. Посчитаем энтропию для случайной величины:

\begin{center}
	\begin{tabular}{c|c|c|c}
		$Y$ & $1$ & $17$  &  $26$  \\ \hline
		$\PP(Y = k)$ & $\frac{1}{3}$ & $\frac{1}{3}$  & $\frac{1}{3}$ 
	\end{tabular}
\end{center}

Энтропия никак не смотрит на то, какие именно значения принимает случайная величина. Её интересует только то, как вероятность размазана по этим значениям:

\[ H(Y) = - \frac{1}{3} \cdot \ln \left(\frac{1}{3}\right)- \frac{1}{3} \cdot \ln \left(\frac{1}{3}\right) - \frac{1}{3} \cdot \ln \left(\frac{1}{3}\right) = \ln 3.\] 

Для случайной величины, принимающей $4$ значения с вероятностями $\frac{1}{4}$ энтропия будет равна $\ln 4$, а в общем случае для $Y \sim U[0;a]$ энтропия составит 

\[H(Y) = \int_0^a \frac{1}{a} \cdot (- \ln\left(\frac{1}{a}\right)) \dx{t} = \ln a. \]

Чем больше значений принимает равномерная случайная величина, тем она непредсказуемее.  Кстати говоря, для вырожденного распределения 

\begin{center}
	\begin{tabular}{c|c}
		$Y$ & $42$   \\ \hline
		$\PP(Y = k)$ & $1$
	\end{tabular}
\end{center}

энтропия окажется нулевой. Вырожденная случайная величина очень даже определена. 

Попробуем более сложную ситуацию, найдём энтропию для нормально распределённой случайной величины $Y \sim N(0, \sigma^2)$:

\begin{multline*}
H(Y) = \E( - \ln(f_Y(y)))  = - \int_{-\infty}^{+\infty} f_Y(y) \cdot \ln f_Y(y) \dx{y} = \\ = \E\left(\frac{1}{2} \ln(2 \pi \sigma^2 ) + \frac{Y^2}{2\cdot \sigma^2}\right) = \frac{1}{2} \ln(2 \pi \sigma^2) + \frac{1}{2}.
\end{multline*} 

Если попробовать подставить в формулу разные значения $\sigma$, то можно получить следущую примерную табличку: 

\begin{center}
	\begin{tabular}{c|c|c|c}
		$\sigma$ & $1$ & $10$  &  $100$  \\ \hline
		$H(Y)$ & $\ln 4.13 $ & $\ln 41.3 $  & $\ln 413 $
	\end{tabular}
\end{center}

Выходит, что случайные величины $X \sim U[0;4]$ и $Y \sim N(0,1)$ в плане непредсказуемости очень похожи.  

Немного забежим вперёд. В байесовских методах с помощью априорного распределения можно выразить какую-то информацию, которую мы знаем о параметре. Часто статистики вообще ничего не знают о параметрах. Это незнание можно выразить либо с помощью равномерного распределения, либо с помощью нормального распределения с большой дисперсией. Энтропия в какой-то степени является обоснованием того, почему эти два подхода замоделировать своё априорное незнание эквивалентны. 

Энтропия обладает несколькими интересными математическими свойствами:

\begin{enumerate}
	\item  Для конечного числа исходов $m$, равномерное распределение над этими исходами будет давать максимальную энтропию. 

	\item Если $X$ и $Y$ независимы, то $H(X \cdot Y) = H(X) + H(Y)$. 
	
	\item Если $X$ и $Y$ имеют одинаковые распределения, но принимают разные значения, то $H(X) = H(Y)$. Энтропия это функция над распределениями, а не значениями. 
\end{enumerate}

На самом деле формула для энтропии была получена как раз исходя из этих свойств\footnote{Помните мы недавно обсуждали как формула для простейшего потока события тоже была получена из свойств этого потока? Можно показать, что эта формула единственна. Тут такая же ситуация. Честно говоря, это восхищает.}.  Освоились? Давайте теперь поговорим про смысл. Начнём с дискретной случайной величины и игры в Акинатора. 

\todo[inline]{Законспектировать}

% https://ru.akinator.com/

% \textbf{Ещё раз, ещё раз.} При использовании в формуле энтропии двоичного логарифма, мы можем интерпретировать её как среднее количество бит информации, которое мы  тратим на кодирование. На практике нам обычно хочется полегче считать энтропию. Поэтому мы отойдём от двоичного основания назад к натуральному, так как с натуральными логарифмами работать намного приятнее. 



Итак, энтропия отражает то, насколько случайная величина непредсказуема. Другой характеристикой, описывающей вариабильность случайной величины является дисперсия. Отличие энтропии от дисперсии в том, что ей плевать на значения, которые принимает случайная величина. Если найти энтропию, для равномерного, нормального и экспоненциального распределений, то мы получим, что:

\begin{equation*}
\begin{aligned}
 N(0,\sigma^2):  \qquad  &H(Y)  = \frac{1}{2} \cdot \ln(2 \pi e) + \ln \sigma \\
 U[a;b]:         \qquad     &H(Y) = \frac{1}{2} \cdot \ln(12) + \ln \sigma,  \quad  \sigma^2 = \frac{1}{12}(b-a)^2 \\ 
 Exp(\alpha):      \qquad &H(Y) = 1 + \ln \sigma, \quad \sigma^2 = \alpha^{-2}\\
\end{aligned}
\end{equation*}

Энтропия для всех трёх распределений выражается через дисперсию и может быть записана, как логарифм стандартного отклонения плюс некоторая константа. Этот факт заставляет начать сомневаться в целесообразности введения нового понятия.

На самом деле с целесообразностью всё в порядке. Например, для мультимодальных распределений обнаруженная нами закономерность нарушается. Можно довольно легко подобрать случайные величины $X$ и $Y$, для которых $\Var(X) > \Var(Y)$, но при этом $H(X) < H(Y)$.

Всё бы хорошо, но это порождает новый вопрос. Какая из случайных величин, $X$ или $Y$ непредсказуемее? Какую метрику для этого использовать? В прочем, ответ вас не удивит. Выбор метрики зависит от задачи. 

\textbf{Ещё раз, ещё раз.}  Вся мощь энтропии заключена в том, что она описывает неопределённость, заложенную в случайную величину абстрагируясь от её значений и порядка этих значений.  Дисперсия, в свою очередь, очень пристально смотрит на значения случайной величины и их порядок. 

Давайте представим себе парочку ситуаций. 

\todo[inline]{Пример когда плевать на значения}

Пусть теперь на наш остров надвигается шторм. Хочется понять будет ли шторм разрушительным. Пусть $X$ это скорость ветра в километрах в час.   Давайте сравним между собой два распределения: 

\begin{enumerate}
\item $X_1 \sim N(100, 10^2)$. 
\item $X_2 \sim $  смесь $N(50, 3^2)$ и $N(200, 3^2)$ с весами $0.5$. 
\end{enumerate}

Как раз в данном случае одно из распределение бимодальное. Мы можем увидеть, что $\Var(X_1) <  \Var(X_2)$, но при этом $H(X_1) >  H(X_2)$, так как случайная величина $X_2$ более плотно сосредоточена на двух конкретных значениях.  С практической точки зрения было бы правильным говорить, что случайная величина $X_2$ несёт в себе большую неопределённость, так как  мы не знаем будет ли обычный ветер или разрушительный шторм. В случае $X_1$ мы знаем, что ветер будет сильным, но неразрушительным. В данном случае имеет смысл руководствоваться дисперсией, так как для нас важны значения, которые принимает случайная величина. 

Обратите внимание на то, что внутри энтропии фигурирует логарифм правдоподобия $\ln f_Y(y)$. Она довольно часто используется в машинном обучении. Например, с помощью неё обучают деревья.  Кроме того, на ней базируется понятие спутанности (perplexity), которое определяется как 

\[ Perplexity(Y) = e^{H(X)}\]

Перплексия (спутанность) довольно часто используется в моделях, связанных с обучением без учителя, но с этим мы встретимся позже. Сейчас наша дорога ведёт нас к очередному новому понятию, \indef{дивергенции Кульбака-Лейблера.}


\section{Дивергенция Кульбака-Лейблера} 

Дивергенция Кульбака-Лейблера пришла в теорию вероятностей из теории информации. По сути \indef{KL-дивергенция ---} это мера разницы между двумя вероятностными распределениями $P$ и $Q$.  Обычно считают, что $P$ --- это истиное распределение, а $Q$ --- его приближение.  Пусть распределение $P$ сложное и страшное и мы хотим заменить его на простое и хорошее. 

\todo[inline]{картинка с простым распределением и со страшным}

В таком случае дивергенция служит оценкой качества приближения и отражает то, какое количество информации мы потеряли, заменив распределение $P$ на распределение $Q$.  Обычно KL-дивергенцию обозначают как $KL(P || Q)$. 

\todo[inline]{Переписать, написано очень хуёво!}

Для дискретного случая дивергенцию можно найти как 

\[ KL(P || Q) = \sum  \PP(x) \cdot \ln \frac{ \PP(x)}{Q(x)}. \]

Для непрерывного случая формула аналогична, но сумма заменяется на интеграл, а вероятности на плотности


\[ KL(P || Q) = \int f_P(x) \cdot \ln \frac{ f_P(x)}{f_Q(x)}. \]

Выше мы сказали, что KL-дивергенция позволяет измерять расстояния между распределениями. Однако уже по формуле видно, что дивергенция Кульбака-Лейблера расстоянием не является.  Из курса матана долгопомнящий читатель может вспомнить, что для того, чтобы функция $\rho(x,y)$ была расстоянием, необходимо выполнение трёх свойств: 

\begin{enumerate}
\item   Неотрицательность: $\rho(x,y) \ge 0$. Расстояние от одного объекта до другого всегда положительно. Если расстояние равно нулю, то объекты находятся в одном месте. 

\item  Симметричность: $\rho(x,y) = \rho(y,x)$. От первого объекта до второго ехать столько же сколько обратно. 

\item  Неравенство треугольника: $\rho(x,y) + \rho(y,z) \ge \rho(x,z)$. Если мы ехали из одной точки в другую и по пути заехали куда-то ещё, то ехать придётся подольше, если наша остановка была нам не по пути. 
\end{enumerate}

С первым свойством всё хорошо. Со вторым и третьим начинаются проблемы. Дивергенция несимметрична, $KL(P || Q) \ne KL(Q || P)$. Зачем так сложно? Почему бы просто не взять и не использовать обычные давно известные метрики. Мало того, что они нам знакомы, так ещё и симметричны. Например, почему бы не взять $\rho(P,Q) = \max_t |f_P(x) - f_Q(x) |$? 

Для ответа на этот вопрос снова посмотрим на парочку картинок.

% \todo[inline]{картинки}

На левой картинке расположены разные распределения. Однако  в терминах $\rho(P,Q)$ эти две плотности будут похожи.  Метрика забьёт на вероятностные различия и сконцентрируется на функциональных особенностях плотностей. На второй картинке наоборот для двух похожих распределений $\rho(P,Q)$ выдаст сильную разницу. Одним словом говоря, дивергенция хорошо зарекомендовала себя на практике даже несмотря на то, что она не является расстоянием в привычном для на смысле. Скорее даже наоборот, ассиметрия помогает нам, так как обычно мы всегда хотим заменить забубенистое распределение, которое мы видим в данных чем-то более простым, а это односторонняя ситуация. 

Поглядим на формулу для поиска энтропии чуть пристальнее и попробуем над ней немного поколдовать

\begin{multline*}
KL(P || Q) = \sum \PP(x) \cdot \ln \frac{ \PP(x)}{Q(x)} = \\ = \sum \PP(x) \ln \PP(x) - \sum \PP(x) \ln Q(x) = H(P) + H(P,Q).
\end{multline*}

Получается, что $KL$-дивергенция в явном виде выражается через энтропию распределения $P$.  Второе слагаемое называется \indef{перекрёстной или кросс энтропией}.  Перекрёстная энтропия интерпретируется как риск использования распределения $Q$, если данные пришли из $P$. 

Выходит, что оценка качества приближения распределения $P$ распределением $Q$ складывается из двух составляющих: энтропии $P$ и риска использования $Q$ вместо $P$. 

\todo[inline]{Тут хуевая часть, которую надо переписать, закаончивается}

Смысл мы обсудили, теперь давайте попробуем пощупать $KL$-дивиргенцию ручками. Пусть у нас есть случайная величина $X$, имеющая распределение $P$. Это распределение кажется нам слишком сложным и мы хотим заменить его на $Q$. 

\begin{center}
	\begin{tabular}{c|c|c|c}
		$X$ & $1$ & $10$  &  $100$  \\ \hline
		$P$ & $\frac{1}{2}$  & $\frac{1}{4}$   &  $\frac{1}{4}$  \\ \hline 
		$Q$  &  $\frac{1}{3}$  &  $\frac{1}{3}$  &  $\frac{1}{3}$ 
	\end{tabular}
\end{center}


Давайте посчитаем дивергенцию между этими двумя распределениями. 

\begin{multline*}
KL(P || Q) = \left[ \frac{1}{2} \ln(3) + \frac{1}{4} \ln(3) + \frac{1}{4} \ln(3) \right] - \left[ \frac{1}{2} \ln(2) + \frac{1}{4} \ln(4) + \frac{1}{4} \ln(4) \right] = 0.059.
\end{multline*}

Теперь сделаем то же самое для непрерывной случайной величины. Найдём дивергенцию между $N(0,1)$ и $N(0,4)$.

\begin{multline*}
KL(N(0,1) || N(0,4)) = \int_{-\infty}^{+\infty} f_P(t) \cdot (-\ln(f_Q(t)) \dx{t} - \\ -\int_{-\infty}^{+\infty} f_P(t) \cdot (-\ln(f_P(t)) \dx{t} = \int_{-\infty}^{+\infty} f_P(t) \cdot \left( -\ln \frac{f_Q(t)}{f_P(t)} \right) \dx{t}. 
\end{multline*}

Найдём второй множитель

\[ - \ln \frac{f_Q(t)}{f_P(t)}  = -\ln \left(\frac{1}{2} \cdot e^{\tfrac{3}{8} t^2} \right) = \ln2 - \frac{3t^2}{8}.  \]

Теперь добьём дивергенцию 

\[ \int_{-\infty}^{+\infty} f_P(t) (\ln2 - \frac{3t^2}{8}) \dx{t} = \ln2 \cdot 1 - \frac{3}{8} \E(X^2) = \ln2 - \frac{3}{8}. \]

Если заменить натуральный логарифм на двоичный, можно снова уйти в биты.  Но обычно для удобства используется именно натуральный логарифм.

\todo[inline]{Пример про классификацию}
 
 В байесовских методах с помощью $KL$-дивергенции можно оценивать информационный выигрыш при переходе от априорного распределения к апостериорному. В таком контексте формула примет вид 
 
 \[ KL(\b \mid y_{n+1}, y || \b \mid y) =  \int f(\b \mid y_{n+1}, y) \cdot \ln \frac{f(\b \mid y_{n+1}, y)}{f(\b \mid y)}. \]

Полученная в ходе вычислений цифра будет отражать то, какое количество дополнительной информации касательно параметра $\b$ мы получили, пронаблюдав дополнительный $y_{n+1}$. Другой путь осознать насколько полезным оказалось новое наблюдение  --- посмотреть на разность апостериорной и априорной энтропий

\[ \Delta H =  H(\b \mid y_{n+1}, y) - H(\b \mid y).\]

Никто не гарантирует, что эта разность получится положительной.  В случае дивергенции Кульбака-Лейблера мы можем быть уверены, что прирост информации окажется либо нулевым либо положительным.  

Здесь мы можем увидеть разницу между информацией и неопределённостью во всей красе. Энтропия измеряет неопределённость. При поступлении нового наблюдения она может как возрасти так и уменьшиться.  $KL$-дивергенция отражает то, сколько дополнительной информации мы извлекли из нового наблюдения. Прирост информации есть всегда. Даже когда она увеличивает неопределённость. 

Конечно же, если говорить в терминах математических ожиданий, то ожидаемый прирост информации всегда равен ожидаемому уменьшению неопределённости.  Конечно же, при условии, что модель была верно специфицирована.  

По аналогии мы можем применять метрики из теории информации для распределения $y_{new}$, которое мы используем при построении прогнозов.

\todo[inline]{снова монетка и подбрасывания, наблюдение за информацией}

\todo[inline]{выше сунуть пример с игрой в акинатора} 

\todo[inline]{подоказывать свойства} 


% \section{Про поимку шпиона} 

% Пришло время срывать маски.  Если на Земле действуют прогрессоры значительно более развитых рас, то в чём будут, скорее всего, состоять их действия? Если правдоподобие хочет, чтобы мы его не заметили, то что оно сделает?

% Вариант первый: скажет, что оно это функция потерь. Случаи, когда происходит подобная метаморфоза, мы рассмотрели выше. Сейчас мы попытаемся словить пару более тонких ситуаций.  Чтобы сделать это, немного поколдуем с оценками максимального правдоподобия, которые мы получали максимизируя функцию правдоподобия: 

% \[ \hb = \argmax_{\b} L(y \mid \b)  = \argmax_{\b} \prod f(y \mid \b).\]

% Обычно мы пользовались логарифмическим правдоподобием. Давайте домножим его на $-1$, и будем искать вместо максимума минимум. Также давайте домножим его на $\frac{1}{n}$. Эта константа ни на что не повлияет, так как мы сможем избавиться от неё после взятия производной, но интерпретации даст много

% \[ \hb =\argmax_{\b} \sum \ln f(y_i \mid \b)  =  \argmin_{\b}  \left( -\frac{1}{n} \sum \ln f(y_i \mid \b)  \right).\]

% Колдуем дальше. Если бы мы могли заглянуть в хрустальный шар и увидеть там истиное значение параметра $\b_0$, тогда бы по закону больших чисел среднее сходилось бы к математическому ожиданию

% \[ -\frac{1}{n} \sum \ln f(y_i \mid \b_0) \to \E(- \ln f(y \mid \b_0)) = H(Y)\]

% и задача максимизации правдоподобия была бы эквивалентна задаче минимизации энтропии. К сожалению, у нас нет хрустального шара. Тем не менее, мы обладаем математикой, в которой издревле завалялся классический трюк. Добавим нужное нам слагаемое. Не будем забывать, что чтобы купить что-нибудь нужное, надо продать что-нибудь ненужное. Вычтем из функции то же самое слагаемое. 

% \begin{multline*}
% \frac{1}{n} \sum -\ln f(y_i \mid \b) = \\ =  \frac{1}{n} \sum (-\ln f(y_i \mid \b) + \ln f(y_i \mid \b_0) - \ln f(y_i \mid \b_0) ) = \\  = \frac{1}{n} \sum \left[log \frac{f(t \mid \b_0)}{f(y \mid \b)} - \ln f(y \mid \b_0) \right].
% \end{multline*}

% Снова используем закон больших чисел и получим, что 

% \begin{multline*}
% \frac{1}{n} \sum \left[log \frac{f(t \mid \b_0)}{f(y \mid \b)} - \ln f(y \mid \b_0) \right] \to KL(f(y \mid \b_0 || y \mid \b)) + H(Y).
% \end{multline*}

% На второе слагаемое мы не можем оказывать никакого влияния своими манипуляциями. Вся наша статистическая работа идёт с $\b$, которое находится в первом слагаемом.  Выходит, что когда мы хотим получить $\b$ максимально близкое к $\b_0$, мы на самом деле минимизируем дивергенцию. 

% Итак, в текущей главе мы посмотрели на несколько различных понятий и методов, которые оказались довольно сильно переплетены между собой\footnote{Потяни за нить, за ней потянется клубок.}.  Мы поговорили о том, к чему приводит использование различных функций потерь в контексте прогнозов, а также ввели несколько новых понятий, перекочевавших в теорию вероятностей из теории информации. Оказалось, что эти понятия тесно связаны с такой классической штукой, как правдоподобие. 

% В следующей главе мы разовьём идеи, связанные с потерями и поговорим про регуляризацию. Немного позже мы вернёмся к дивергенции и обсудим такую важную штуку, как вариационные приближения. Будет интересно. Но давайте не будем забегать вперёд и для начала прорешаем упражнения к этой главе.  В них надо будет распутать пару клубков. 

% \todo[inline]{мем со скуби-ду и типо вот кто ты на самом деле}

\section{Ещё задачи}

К этой главе по аналогии со сходимостями собрано немного задач, которые можно порешать. Развлекайтесь\footnote{Большая часть задач отсюда: \url{https://github.com/bdemeshev/mlearn_pro}}. 

\begin{problem}{}
Случайная величина $Y$ принимает два значения: $0$ с вероятностью $p$ и $1$ с вероятностью $1-p$. Постройте график зависимости энтропии от $p$. При каком $p$ энтропия будет максимальной? Проинтерпретируйте это. Является ли функция монотонной? Выпуклой? 	
\end{problem}
% \begin{sol} 
% 	Мораль в том, что энторпия измеряет то, насколько плохо прогнозируется случайная величина. Видно, что в точке $0.5$ она предсказуема хуже всего. 
% \end{sol} 

\begin{problem}{ }
Найдите энтропию $X$, спутанность (perplexity) $X$, индекс Джини $X$, если
\begin{enumerate}
  \item величина $X$ равновероятно принимает значения $1$, $7$ и $9$;
  \item величина $X$ равновероятно принимает $k\geq 2$ значений;
  \item величина $X$ равномерно распределена на отрезке $[0;a]$;
  \item величина $X$ нормальна $N(\mu;\sigma^2)$;
\end{enumerate}
\end{problem}

% \begin{sol}
%   \begin{enumerate}
%   \item $H(X) = \ln 3$, $I_X = 2/3$, спутанность равна $3$.
%   \item $I_X = 1-\frac{1}{k}$, $H(X) = \ln k$, спутанность равна $k$.
%   \item Если величина $X$ равновероятно принимает $k$ значений, то спутанность равна $k$. У равномерной на $[0;a]$ спутанность равна $a$. $H(X) = -\int_0^a \frac{1}{a} \cdot \ln \frac{1}{a} dx = \ln a$.
%   \item Обозначим $f(x) = \frac{1}{\sqrt{2 \pi \sigma^2}} \exp\left(-\frac{(x-\mu)^2}{2\sigma^2}\right)$, тогда $H(X) = -\int_{-\infty}^{+\infty} f(x) \ln (f(x)) dx$.
%  \end{enumerate}
% \end{sol}

\begin{problem}{}
Илон Маск прилетел на Марс подбрасывает монетку до первого орла много раз подряд. $X_1$ --- то, сколько было сделано подкидываний до первого орла, $X_2$ --- до второго, $X_3$ --- до третьего и т.д. 

Илон хочет передать информацию о своих опытах на Землю. Какое минимальное количество бит ему нужно, чтобы передать информацию? Опишите в яном виде стратегию её передачи. 
\end{problem}


\begin{problem}{}
У Васи была дискретная случайная величина $X$, принимавшая натуральные значения. Вася решил изменить закон распределения величины $X$. Он увеличил количество возможных значений величины $X$ в два раза, разделив каждое событие $X=k$ на два равновероятных подсобытия: $X=k-0.1$ и $X=k+0.1$. Как при этом изменились энтропия, спутанность (perplexity) и индекс Джини?
\end{problem}

% \begin{sol}
% Энтропия, спутанность (perplexity) и индекс Джини вырастут.
% \end{sol}

\begin{problem}{}
Кот исследователя Василия случайно нажимает на клавиатуре клавиши А, Б и В $n$ раз. Коту больше нравится клавиша А, поэтому вероятность этой буквы равна $1/2$, а у Б и В вероятности равны по $1/4$. Василий хочет заархивировать послание Кота для потомков, ведь, возможно, в послании кроется Великий Смысл. Поскольку буква А встречается чаще, Василий кодирует её более короткой последовательностью битов, а именно, одним битом 0. Букву Б Василий кодирует кодом 10, а букву В — кодом 11.

\begin{enumerate}
\item Найдите ожидаемое отношение длины заархивированного сообщения к количеству букв.
\item Докажите, что код, предложенный Василием, имеет наименьшую ожидаемую длину архива.
\item Найдите энтропию с логарифмом по основанию 2 и спутанность (perplexity) для нажатия Котом одной буквы.
\end{enumerate}
\end{problem}

% \begin{sol}
% \begin{enumerate}
% \item Пусть $X$ — длина заархивированного сообщения, $X = X_1 + ... + X_n$, где $X_i$ — длина одной заархивированной буквы.
% \[
% \E(X) = n \E(X_1) = n \left(\frac{1}{2} \cdot 1 + \frac{1}{4} \cdot 2 + \frac{1}{4} \cdot 2 \right) = n \cdot \frac{3}{2} \Rightarrow \frac{\E(X)}{n} = \frac{3}{2}
% \]
% \item[3.] $H(X) = -\left(\frac{1}{2}\log_2 \frac{1}{2} + \frac{1}{4} \log_2 \frac{1}{4} + \frac{1}{4} \log_2 \frac{1}{4} \right) = \frac{3}{2}$
% \end{enumerate}
% \end{sol}

\begin{problem}{}
Как изменится энтропия дискретной величины $X$, если величину домножить на 10? А если у величины $X$ есть функция плотности?
\end{problem}

% \begin{sol}
% У дискретной величины энтропия не изменится.
% \end{sol}

\begin{problem}{}
Как связаны энтропия и дисперсия для равномерного, экспоненциального и
нормального распределений? 
\end{problem}

% \begin{sol}
% Энтропия линейно зависит от логарифма дисперсии.
% \end{sol}

\begin{problem}{}
Найдите дивергенцию Кульбака-Лейбнера, если она определена, 

\begin{enumerate}
\item  из биномиального $Bin(\frac{1}{3}, 2)$ в равновероятное на $0,1,2$;
\item из равновероятного на $0,1,2$ в биномиального $Bin(\frac{1}{3}, 2)$;
\item из $N(0,l)$ в $N(0, \sigma^2)$;
\item из  $N(0, \sigma^2)$ в $N(0,l)$;
\item из $N(0,1)$ в $Exp(1)$;
\item из $Exp(1)$ в $N(0,l)$. 
\end{enumerate}
\end{problem}








\end{document}